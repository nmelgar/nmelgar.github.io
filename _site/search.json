[
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 4"
    ]
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 2"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "Physicist, Mathematician, Cambridge professor.\n\nisaac@applesdofall.org | My wikipedia page\n\n\n\nStanding on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples.\n\n\n\n\n1654-1660 The King’s School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow\n\n\n\n\n2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France\n\n\n\n\n\n\n1669 Newton Sir I, De analysi per æquationes numero terminorum infinitas.\n1669 Lectiones opticæ.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001\n\n\n\n\n1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "Standing on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "1654-1660 The King’s School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow"
  },
  {
    "objectID": "resume.html#awards",
    "href": "resume.html#awards",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "1669 Newton Sir I, De analysi per æquationes numero terminorum infinitas.\n1669 Lectiones opticæ.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "Survey data is notoriously difficult to munge. Even when the data is recorded cleanly the options for ‘write in questions’, ‘choose from multiple answers’, ‘pick all that are right’, and ‘multiple choice questions’ makes storing the data in a tidy format difficult.\nIn 2014, FiveThirtyEight surveyed over 1000 people to write the article titled, America’s Favorite ‘Star Wars’ Movies (And Least Favorite Characters). They have provided the data on GitHub.\nFor this project, your client would like to use the Star Wars survey data to figure out if they can predict an interviewing job candidate’s current income based on a few responses about Star Wars movies.\n\n\nRead and format project data\nurl = \"https://github.com/fivethirtyeight/data/blob/master/star-wars-survey/StarWars.csv?raw=true\"\ndata_sw = pd.read_csv(url, encoding=\"ISO-8859-1\")\ndata_sw = data_sw.drop(index=0)"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#elevator-pitch",
    "href": "Cleansing_Exploration/project5.html#elevator-pitch",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "Survey data is notoriously difficult to munge. Even when the data is recorded cleanly the options for ‘write in questions’, ‘choose from multiple answers’, ‘pick all that are right’, and ‘multiple choice questions’ makes storing the data in a tidy format difficult.\nIn 2014, FiveThirtyEight surveyed over 1000 people to write the article titled, America’s Favorite ‘Star Wars’ Movies (And Least Favorite Characters). They have provided the data on GitHub.\nFor this project, your client would like to use the Star Wars survey data to figure out if they can predict an interviewing job candidate’s current income based on a few responses about Star Wars movies.\n\n\nRead and format project data\nurl = \"https://github.com/fivethirtyeight/data/blob/master/star-wars-survey/StarWars.csv?raw=true\"\ndata_sw = pd.read_csv(url, encoding=\"ISO-8859-1\")\ndata_sw = data_sw.drop(index=0)"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#question-1",
    "href": "Cleansing_Exploration/project5.html#question-1",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION 1",
    "text": "QUESTION 1\nSHORTEN THE COLUMNS AND CLEAN THEM UP FOR EASIER USE WITH PANDAS\nWe wrangled the messy Star Wars survey data by shortening and cleaning column names for smoother analysis with pandas. We are going to see a glimpse of the transformation:\n\n\nCode to analyze\n# Code to execute\n# create a df to display the names of the columns olds and news\nnew_columns_df = pd.DataFrame(columns=[\"old_columns\", \"new_columns\"])\nold_cols = data_sw.columns.tolist()\n# insert old column names from data_sw to new 'old_columns' column in new cols df\nnew_columns_df[\"old_columns\"] = old_cols\n\ndef new_columns(df_frame):\n    # new names for the columns\n    df_frame.columns = [\n    \"respondent_id\",\n    \"seen_any_sw_film\",\n    \"sw_fan\",\n    \"seen_ep1\",\n    \"seen_ep2\",\n    \"seen_ep3\",\n    \"seen_ep4\",\n    \"seen_ep5\",\n    \"seen_ep6\",\n    \"rank_ep1\",\n    \"rank_ep2\",\n    \"rank_ep3\",\n    \"rank_ep4\",\n    \"rank_ep5\",\n    \"rank_ep6\",\n    \"fav_han\",\n    \"fav_luke\",\n    \"fav_leia\",\n    \"fav_anakin\",\n    \"fav_obi\",\n    \"fav_emperor\",\n    \"fav_darth\",\n    \"fav_lando\",\n    \"fav_boba\",\n    \"fav_c3po\",\n    \"fav_r2d2\",\n    \"fav_jarjar\",\n    \"fav_padme\",\n    \"fav_yoda\",\n    \"shot_first\",\n    \"expanded_universe_fam\",\n    \"expanded_universe_fan\",\n    \"star_trek_fan\",\n    \"gender\",\n    \"age\",\n    \"income\",\n    \"education\",\n    \"location\",\n]\nnew_columns(data_sw)\n# insert new column names in the new cols df\nnew_columns_df[\"new_columns\"] = data_sw.columns.tolist()\n\nnew_columns_df.head(10)\n\n\n\n\n\n\n\n\n\nold_columns\nnew_columns\n\n\n\n\n0\nRespondentID\nrespondent_id\n\n\n1\nHave you seen any of the 6 films in the Star W...\nseen_any_sw_film\n\n\n2\nDo you consider yourself to be a fan of the St...\nsw_fan\n\n\n3\nWhich of the following Star Wars films have yo...\nseen_ep1\n\n\n4\nUnnamed: 4\nseen_ep2\n\n\n5\nUnnamed: 5\nseen_ep3\n\n\n6\nUnnamed: 6\nseen_ep4\n\n\n7\nUnnamed: 7\nseen_ep5\n\n\n8\nUnnamed: 8\nseen_ep6\n\n\n9\nPlease rank the Star Wars films in order of pr...\nrank_ep1\n\n\n\n\n\n\n\nColumns names were changed for all the dataframe so it can be easier to understand it."
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#question-2",
    "href": "Cleansing_Exploration/project5.html#question-2",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION 2",
    "text": "QUESTION 2\nCLEAN AND FORMAT THE DATA SO THAT IT CAN BE USED IN A MACHINE LEARNING MODEL.\nAs part of the cleaning we are going to complete each of the items listed below, by doing this the data will be better formatted and prepared to use in a ML model: - Tasks to complete: - Filter the dataset to respondents that have seen at least one film - Create a new column that converts the age ranges to a single number. Drop the age range categorical column - Create a new column that converts the education groupings to a single number. Drop the school categorical column - Create a new column that converts the income ranges to a single number. Drop the income range categorical column - Create your target (also known as “y” or “label”) column based on the new income range column - One-hot encode all remaining categorical columns\n\n\nCode to clean up the data\n# add respondent id an easier to read number\ndata_sw[\"respondent_id\"] = range(1, len(data_sw) + 1)\n\n#     Filter the dataset to respondents that have seen at least one film\ndata_sw = data_sw[data_sw[\"seen_any_sw_film\"] == \"Yes\"]\n\n# fill NaN for numerical columns usign the median\nnum_cols = data_sw.select_dtypes(include=[\"float64\", \"int64\"]).columns\nfor col in num_cols:\n    data_sw[col] = data_sw[col].fillna(data_sw[col].median())\n\n# fill NaN for categorical columns using the mode\ncat_cols = data_sw.select_dtypes(include=[\"object\"]).columns\nfor col in cat_cols:\n    data_sw[col] = data_sw[col].fillna(data_sw[col].mode()[0])\n\n# Create a new column that converts the age ranges to a single number. Drop the age range categorical column\ndef convert_age(age_range):\n    if pd.isnull(age_range):\n        return None\n    age_mapping = {\n        \"18-29\": 24,\n        \"30-44\": 37,\n        \"45-60\": 53,\n        \"&gt; 60\": 65,\n    }\n    return age_mapping.get(age_range, None)\n\ndata_sw[\"age_range\"] = data_sw[\"age\"].apply(convert_age)\ndata_sw = data_sw.drop(columns=[\"age\"])\n\n# Create a new column that converts the education groupings to a single number. Drop the school categorical column\ndef convert_education(education):\n    if pd.isnull(education):\n        return None\n    edu_mapping = {\n        \"Less than high school degree\": 1,\n        \"High school degree\": 2,\n        \"Some college or Associate degree\": 3,\n        \"Bachelor degree\": 4,\n        \"Graduate degree\": 5,\n    }\n    return edu_mapping.get(education, None)\n\ndata_sw[\"education_group\"] = data_sw[\"education\"].apply(convert_education)\ndata_sw = data_sw.drop(columns=[\"education\"])\n\n# Create a new column that converts the income ranges to a single number. Drop the income range categorical column\ndef convert_income(income_range):\n    if pd.isnull(income_range):\n        return None\n    income_mapping = {\n        \"$0 - $24,999\": 125000,\n        \"$25,000 - $49,999\": 375000,\n        \"$50,000 - $99,999\": 75000,\n        \"$100,000 - $149,999\": 125000,\n        \"$150,000+\": 150000,\n    }\n    return income_mapping.get(income_range, None)\n\ndata_sw[\"income_group\"] = data_sw[\"income\"].apply(convert_income)\ndata_sw = data_sw.drop(columns=[\"income\"])\n\n# Create your target (also known as “y” or “label”) column based on the new income range column\ndata_sw[\"target_income\"] = data_sw[\"income_group\"].apply(\n    lambda x: 1 if x &gt; 50000 else 0\n)\n# One-hot encode all remaining categorical columns\ndata_sw = pd.get_dummies(data_sw, columns=[\"sw_fan\", \"gender\", \"location\"])\n\ndata_sw.head(2)\n\n\n\n\n\n\n\n\n\nrespondent_id\nseen_any_sw_film\nseen_ep1\nseen_ep2\nseen_ep3\nseen_ep4\nseen_ep5\nseen_ep6\nrank_ep1\nrank_ep2\n...\ngender_Male\nlocation_East North Central\nlocation_East South Central\nlocation_Middle Atlantic\nlocation_Mountain\nlocation_New England\nlocation_Pacific\nlocation_South Atlantic\nlocation_West North Central\nlocation_West South Central\n\n\n\n\n1\n1\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n3\n2\n...\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n3\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n1\n2\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n2 rows × 49 columns\n\n\n\nColumns were cleaned, NaN were filled and new columns were added."
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#question-3",
    "href": "Cleansing_Exploration/project5.html#question-3",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION 3",
    "text": "QUESTION 3\nMachine Learning Model whether a person makes more than $50k\nWe are going to build a Machine Learning model to predict whether a person makes more than $50k.\nThe machine learning model created for this project is a Random Forest Classifier, designed to predict whether a person makes more than $50k based on their responses to the Star Wars survey. The dataset was preprocessed to handle missing values, encode categorical variables, and convert age, education, and income ranges into numerical values.\n\n\nCode to display create ML model\n# split the data in features and target label\nX = data_sw.drop(\n    columns=[\n        \"target_income\",\n        \"seen_any_sw_film\",\n        \"seen_ep1\",\n        \"seen_ep2\",\n        \"seen_ep3\",\n        \"seen_ep4\",\n        \"seen_ep5\",\n        \"seen_ep6\",\n        \"fav_han\",\n        \"fav_luke\",\n        \"fav_leia\",\n        \"fav_anakin\",\n        \"fav_obi\",\n        \"fav_emperor\",\n        \"fav_darth\",\n        \"fav_lando\",\n        \"fav_boba\",\n        \"fav_c3po\",\n        \"fav_r2d2\",\n        \"fav_jarjar\",\n        \"fav_padme\",\n        \"fav_yoda\",\n        \"shot_first\",\n        \"expanded_universe_fam\",\n        \"expanded_universe_fan\",\n        \"star_trek_fan\",\n    ]\n)\ny = data_sw[\"target_income\"]\n\n# split data in training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# initialize the random forest classifier\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train, y_train)\ny_pred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\n# show the accuracy\nprint(f\"The accuracy of the Random Forest Classifier is: {accuracy:.2f}\")\nprint(classification_report(y_test, y_pred))\n\n\nThe accuracy of the Random Forest Classifier is: 1.00\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00       281\n\n    accuracy                           1.00       281\n   macro avg       1.00      1.00      1.00       281\nweighted avg       1.00      1.00      1.00       281\n\n\n\nThis report mentions a perfect accuracy of 100%, with a support of 281."
  },
  {
    "objectID": "Cleansing_Exploration/project5.html#question-4",
    "href": "Cleansing_Exploration/project5.html#question-4",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION 4",
    "text": "QUESTION 4\nVALIDATE THAT DATA PROVIDED LINES UP WITH THE ARTICLE\nWe are going to validate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the original article, that will let us know if it makes sense of not.\nBased on the “Who Shot First” column a chart was recreated, showing the character percentages for this column.\n\n\nCode to display\nurl = \"https://github.com/fivethirtyeight/data/blob/master/star-wars-survey/StarWars.csv?raw=true\"\ndata_sw = pd.read_csv(url, encoding=\"ISO-8859-1\")\ndata_sw = data_sw.drop(index=0)\nnew_columns(data_sw)\nshot_first_counts = data_sw[\"shot_first\"].value_counts(normalize=True) * 100\nshot_first_counts = shot_first_counts.reset_index()\nshot_first_counts.columns = [\"shot_first\", \"percentage\"]\n\n# round to whole number\nshot_first_counts[\"percentage\"] = shot_first_counts[\"percentage\"].round(0)\n\n# create chart\nfig = px.bar(\n    shot_first_counts,\n    y=\"shot_first\",\n    x=\"percentage\",\n    title=\"Who Shot First?\",\n    labels={\"shot_first\": \"Character who shot first\", \"percentage\": \"Percentage\"},\n    text=\"percentage\",\n    orientation=\"h\",\n)\n\nfig.show()\n\n\n                                                \n\n\nThis chart show the same percentages than the article, showing consistency with the data.\nWe then asked respondents which of the films they had seen. With 835 people responding, here’s the probability that someone has seen a given “Star Wars” film given that they have seen any Star Wars film.\n\n\nCode to create chart for those who have seen sw movies\ndata_sw = data_sw[data_sw[\"seen_any_sw_film\"] == \"Yes\"]\n\nstar_wars_movies_list = [\n    \"The Phantom Menace\",\n    \"Attack of the Clones\",\n    \"Revenge of the Sith\",\n    \"A New Hope\",\n    \"The Empire Strikes Back\",\n    \"Return of the Jedi\",\n]\n\nsee_movies_list = [\n    \"seen_ep1\",\n    \"seen_ep2\",\n    \"seen_ep3\",\n    \"seen_ep4\",\n    \"seen_ep5\",\n    \"seen_ep6\",\n]\n\nseen_film_counts = pd.DataFrame(columns=[\"Movie\", \"Percentage\"])\n\npercentages = []\nfor seen in see_movies_list:\n    percentage = data_sw[seen].notna().mean() * 100\n    percentages.append(percentage)\n\nseen_film_counts = pd.DataFrame(\n    {\"Movie\": star_wars_movies_list, \"Percentage\": percentages}\n)\n\nseen_film_counts[\"Percentage\"] = seen_film_counts[\"Percentage\"].round(0)\n\n# create chart\nseen_fig = px.bar(\n    seen_film_counts,\n    y=\"Movie\",\n    x=\"Percentage\",\n    title=\"Which 'Star Wars' Movie Have You Seen?\",\n    text=\"Percentage\",\n    orientation=\"h\",\n)\n\nseen_fig.show()\n\n\n                                                \n\n\nThis chart shows different percentages than the article, even when filtered."
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - Finding relationships in baseball",
    "section": "",
    "text": "When you hear the word “relationship” what is the first thing that comes to mind? Probably not baseball. But a relationship is simply a way to describe how two or more objects are connected. There are many relationships in baseball such as those between teams and managers, players and salaries, even stadiums and concession prices. Let’s analyze other relationships in baseball.\n\n\nRead and format project data\nsqlite_file = \"lahmansbaseballdb.sqlite\"\ndb = sqlite3.connect(sqlite_file)"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#elevator-pitch",
    "href": "Cleansing_Exploration/project3.html#elevator-pitch",
    "title": "Client Report - Finding relationships in baseball",
    "section": "",
    "text": "When you hear the word “relationship” what is the first thing that comes to mind? Probably not baseball. But a relationship is simply a way to describe how two or more objects are connected. There are many relationships in baseball such as those between teams and managers, players and salaries, even stadiums and concession prices. Let’s analyze other relationships in baseball.\n\n\nRead and format project data\nsqlite_file = \"lahmansbaseballdb.sqlite\"\ndb = sqlite3.connect(sqlite_file)"
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#question-1",
    "href": "Cleansing_Exploration/project3.html#question-1",
    "title": "Client Report - Finding relationships in baseball",
    "section": "QUESTION 1",
    "text": "QUESTION 1\nHAS ANY PROFESSIONAL BASEBALL PLAYER ATTENDED TO BYU?\nIn the following table we can see those professional players who attended BYU-I, their salary, the year they played for a professional team and the team’s id.\nDoes a 4000000 salary look bad? I don’t think so."
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#question-2",
    "href": "Cleansing_Exploration/project3.html#question-2",
    "title": "Client Report - Finding relationships in baseball",
    "section": "QUESTION 2",
    "text": "QUESTION 2\nWHAT PLAYERS HAVE GOTTEN THE HIGHEST BATTING AVERAGE OVER THEIR ENTIRE CAREERS?\nThe batting average is the number of hits divided by the number of at-bats of the player. If a player has a high number of at-bats it doesn’t mean he will have a high number of hits, they will vary from player to player, let’s analyz more of this.\n\n\nCode to batting average w/at least 1 at-bat\n# Include code here\n# hits = \"\"\"SELECT h, ab, playerid, yearid FROM batting WHERE ab &gt;= 1\"\"\"\n# hits_total = pd.read_sql_query(hits, db)\n# hits_total[\"batting_avg\"] = hits_total[\"H\"] / hits_total[\"AB\"]\n# ab1_filtered = hits_total[[\"playerID\", \"yearID\", \"batting_avg\"]]\n# ab1_filtered = ab1_filtered.sort_values(\n#     by=[\"batting_avg\", \"playerID\"], ascending=[False, True]\n# )\n# ab1_filtered.head(5)\n\n\nIt seems the amount of players with a batting average of 1 is high.\n\n\nCode to batting average w/at least 10 at-bat\n# Include code here\n# hits = \"\"\"SELECT h, ab, playerid, yearid FROM batting WHERE ab &gt;= 10\"\"\"\n# hits_total = pd.read_sql_query(hits, db)\n# hits_total[\"batting_avg\"] = hits_total[\"H\"] / hits_total[\"AB\"]\n# ab10_filtered = hits_total[[\"playerID\", \"yearID\", \"batting_avg\"]]\n# ab10_filtered = ab10_filtered.sort_values(\n#     by=[\"batting_avg\", \"playerID\"], ascending=[False, True]\n# )\n# ab10_filtered.head(5)\n\n\nBatting average has decreased as the at bats increased.\n\n\nCode to batting average w/at least 100 at-bat\n# Include code here\n# hits = \"\"\"SELECT h, ab, playerid, yearid FROM batting WHERE ab &gt;= 100\"\"\"\n# hits_total = pd.read_sql_query(hits, db)\n# hits_total[\"batting_avg\"] = hits_total[\"H\"] / hits_total[\"AB\"]\n# ab100_filtered = hits_total[[\"playerID\", \"batting_avg\"]]\n# ab100_filtered = ab100_filtered.groupby(\"playerID\").mean()\n# ab100_filtered = ab100_filtered.sort_values(\n#     by=[\"batting_avg\", \"playerID\"], ascending=[False, True]\n# )\n# ab100_filtered.head(5)\n\n\nBatting average seems to be low, but these players have more than 100 at-bats."
  },
  {
    "objectID": "Cleansing_Exploration/project3.html#question-3",
    "href": "Cleansing_Exploration/project3.html#question-3",
    "title": "Client Report - Finding relationships in baseball",
    "section": "QUESTION 3",
    "text": "QUESTION 3\nIS THERE A BIG DIFFERENCE FOR HOME RUNS NUMBERS WHEN COMPARING BIG AND FAMOUS TEAMS?\nThere have been many baseball teams since this beautiful sport started, many of them still making history and adding numbers to their statistics, let’s compare 2 of these teams. We will pay special attention to the number of home runs. Team 1 will be Cincinnati Reds, and team 2 will be Pittsburgh Pirates.\nBoth teams started registering home runs around the same years, before 1880, Pittsburgh started with a high amount of home runs compared to Cincinnati, but for some reason Cincinnati has a higher amount of home runs through the years, they have a clear advantage.\n\n\nCode to compare CIN vs PIT home runs\n# select all teams\n# teams = \"\"\"SELECT * FROM teams\"\"\"\n# teams_total = pd.read_sql_query(teams, db)\n# teams_total\n\n# # select first team Cincinnati Reds\n# select_teams = \"\"\"SELECT * FROM teams WHERE name = 'Cincinnati Reds' OR name = 'Pittsburgh Pirates'\"\"\"\n# chosen_teams = pd.read_sql_query(select_teams, db)\n# chosen_teams[\"Year\"] = pd.to_datetime(chosen_teams[\"yearID\"], format=\"%Y\")\n\n# chosen_teams\n\n# chart_1 = px.scatter(\n#     chosen_teams,\n#     x=\"Year\",\n#     y=\"HR\",\n#     color=\"name\",\n#     color_discrete_sequence=[\"red\", \"yellow\"],\n#     labels={\"HR\": \"Home Runs\", \"name\": \"Team\"},\n#     title=\"Home Runs through the years\",\n# )\n# chart_1.show()\n\n\nThis table shows the home runs through the years for both teams."
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - What’s in a name?",
    "section": "",
    "text": "Names as ways to identify individuals have been used through world’s history. This project involves data exploration, visualization, and interpretation to gain insights into the trends and patterns related to different names over time. Is Brittany a name for old or young ladies? Has Luke Skywalker from Star Wars influeced the names of a generation? Let’s dive in to the results.\n\n\nRead and format project data\nnames_url = \"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\"\nnames = pd.read_csv(names_url)"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html#elevator-pitch",
    "href": "Cleansing_Exploration/project1.html#elevator-pitch",
    "title": "Client Report - What’s in a name?",
    "section": "",
    "text": "Names as ways to identify individuals have been used through world’s history. This project involves data exploration, visualization, and interpretation to gain insights into the trends and patterns related to different names over time. Is Brittany a name for old or young ladies? Has Luke Skywalker from Star Wars influeced the names of a generation? Let’s dive in to the results.\n\n\nRead and format project data\nnames_url = \"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\"\nnames = pd.read_csv(names_url)"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html#question-1",
    "href": "Cleansing_Exploration/project1.html#question-1",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION 1",
    "text": "QUESTION 1\nHOW DOES YOUR NAME AT YOUR BIRTH YEAR COMPARE TO ITS USE HISTORICALLY?\nMy name was not listed in the data set. So I took my eldest brother’s name which is Matthew, he was born in 1993. Around 1940, Matthew started to increase in popularity, it took 50 years to reach the highest point with more than 35,000 Matthews.. Once the highest point was reached, the name had a decrease in popularity, before 2000 it shew a spark of popularity, but the dicrease continued..\n\n\nCode to analyze Matthew name through the years\nmatthew_data = names.query('name == \"Matthew\"')\n\n# filter data\nperson_name = \"Matthew\"\nyear_range = \"year &gt;= 1910 and year &lt;= 2015\"\nmatthew_data = names.query(f\"name == '{person_name}' and {year_range}\")\n\n# specific year for his born\nborn_year = 1993\nborn_year_data = matthew_data.query(f\"year == {born_year}\")\nspecific_year = born_year_data[\"year\"].values\ntotals_specific_year = born_year_data[\"Total\"].values\n\n# create matthew line chart through the years\nmat_chart = px.line(\n    matthew_data,\n    x=\"year\",\n    y=\"Total\",\n    labels={\"year\": \"Year\"},\n    title=\"Matthew name through the years\",\n)\n\n# add mark to highligth the year when he was born\nmat_chart.add_trace(\n    go.Scatter(\n        x=specific_year,\n        y=totals_specific_year,\n        mode=\"markers\",\n        marker_size=5,\n        name=\"1993\",\n    )\n)\nmat_chart.show()\n\n\n                                                \n\n\n1993 year is highlighted as the year my brother born..\n\n\nTable example\nprint(matthew_data.head(5).filter([\"name\", \"year\", \"Total\"]).to_markdown(index=False))\n\n\n| name    |   year |   Total |\n|:--------|-------:|--------:|\n| Matthew |   1910 |     168 |\n| Matthew |   1911 |     176 |\n| Matthew |   1912 |     357 |\n| Matthew |   1913 |     457 |\n| Matthew |   1914 |     576 |"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html#question-2",
    "href": "Cleansing_Exploration/project1.html#question-2",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION 2",
    "text": "QUESTION 2\nIF YOU TALKED TO SOMEONE NAMED BRITTANY ON THE PHONE, WHAT IS YOUR GUESS OF HIS OR HER AGE? WHAT AGES WOULD YOU NOT GUESS??\ntype your results and analysis here\n\n\nCode to analyze Brittany’s data\n# get data and filter it by name\nbrittany_data = names.query('name == \"Brittany\"')\ncurrent_year = datetime.datetime.now().year\n\n# create a copy of the data to avoid the SettingWithCopyWarning:\nbrittany_data_copy = brittany_data.copy()\nbrittany_data_copy[\"currentAge\"] = current_year - brittany_data_copy[\"year\"]\n\n# create brittany scatter\nbrittany_chart = px.scatter(\n    brittany_data_copy,\n    x=\"currentAge\",\n    y=\"Total\",\n    labels={\n        \"currentAge\": \"Current Age\",\n    },\n    title=\"Brittany's ages in the US\",\n)\nbrittany_chart.show()\n\n\n                                                \n\n\nUsing 2024 as year of reference, Brittany was a popular name many years ago..\n\n\nTable example\n# Include and execute your code here\nprint(brittany_data.head(5).filter([\"name\", \"year\", \"Total\"]).to_markdown(index=False))\n\n\n| name     |   year |   Total |\n|:---------|-------:|--------:|\n| Brittany |   1968 |       5 |\n| Brittany |   1969 |      12 |\n| Brittany |   1970 |      32 |\n| Brittany |   1971 |      81 |\n| Brittany |   1972 |     158 |"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html#question-3",
    "href": "Cleansing_Exploration/project1.html#question-3",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION 3",
    "text": "QUESTION 3\nMARY, MARTHA, PETER, AND PAUL ARE ALL CHRISTIAN NAMES. FROM 1920 - 2000, COMPARE THE NAME USAGE OF EACH OF THE FOUR NAMES. WHAT TRENDS DO YOU NOTICE??\nThese names had an interesting increase in popularity around 1940’s, but for the 1960’s all of them were in their way to decrase in popularity. Mary shows the highest amount of decrease, in around 20 years it went from 45,000 to 15,000, a loss of 30,000, a considerable amount compared to the other names.\n\n\nCode to analyze Chrstian names\n# create copy of the data with specified names and year range\nlist_names = [\"Mary\", \"Martha\", \"Peter\", \"Paul\"]\nyear_names = \"year &gt;= 1920 and year &lt;= 2000\"\nchristian_names_data = names.query(f\"name in @list_names and {year_names}\")\n\n# create line chart to display names between 1920 and 2000\nchristian_names_chart = px.line(\n    christian_names_data,\n    x=\"year\",\n    y=\"Total\",\n    color=\"name\",\n    labels={\n        \"year\": \"Year\",\n        \"name\": \"Name\",\n    },\n    title=\"Christian Names Between 1920-2000\",\n)\nchristian_names_chart.show()\n\n\n                                                \n\n\n“Mary” shows an interesting decrease before 1960.\n\n\nTable for Christian names\nprint(\n    christian_names_data.head(5)\n    .filter([\"name\", \"year\", \"Total\"])\n    .to_markdown(index=False)\n)\n\n\n| name   |   year |   Total |\n|:-------|-------:|--------:|\n| Martha |   1920 |    8705 |\n| Martha |   1921 |    9254 |\n| Martha |   1922 |    9018 |\n| Martha |   1923 |    8731 |\n| Martha |   1924 |    9163 |"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html#question-4",
    "href": "Cleansing_Exploration/project1.html#question-4",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION 4",
    "text": "QUESTION 4\nTHINK OF A UNIQUE NAME FROM A FAMOUS MOVIE. PLOT THE USAGE OF THAT NAME AND SEE HOW CHANGES LINE UP WITH THE MOVIE RELEASE. DOES IT LOOK LIKE THE MOVIE HAD AN EFFECT ON USAGE?\nStar Wars is a popular series of movies, streamings, videogames and more. Luke Skywalker was the main character for the movies IV (1977), V (1980) and VI (1983). Although “Luke” name started to grow in popularity around 1970, from 1977 to 1980 it went from 1235 to 3108 a growth of 252% in just 3 years, after that the name’s popularity had it’s ups and downs.\n\n\nCode to analyze Luke name from Star Wars\n# choose name, and year range for the star wars movies, \"luke\" as the name\nmovie_name = \"Luke\"\nyear_range = \"year &gt;= 1960 and year &lt;= 1990\"\nmovie_name_data = names.query(f\"name == '{movie_name}' and {year_range}\")\n\n# create line chart for luke name through the years\nluke_name_chart = px.line(\n    movie_name_data,\n    x=\"year\",\n    y=\"Total\",\n    labels={\"year\": \"Year\"},\n    title=\"Luke Skywalker impact on names of the people\",\n)\n\n# data to display movies released data, movies iv, v and vi\nmovie_release_year = [1977, 1980, 1983]\nmovie_release_data = names.query(\n    f\"year in @movie_release_year and name == '{movie_name}'\"\n)\ntotals_movie_year = movie_release_data[\"Total\"].values\n\n# add markers for the years when the moview IV to VI were released\nluke_name_chart.add_trace(\n    go.Scatter(\n        x=movie_release_year,\n        y=totals_movie_year,\n        mode=\"markers\",\n        marker_size=8,\n        marker_symbol=\"star\",\n        marker_color=\"red\",\n        name=\"Years when Star Wars movies were released (IV - VI)\",\n    )\n)\n# change the layout so the legend show at bottom of the plot\nluke_name_chart.update_layout(\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=-0.4,\n    )\n)\nluke_name_chart.show()\n\n\n                                                \n\n\nRelease year for movies IV, V and VI are highlighted..\n\n\nTable of years\nprint(\n    movie_release_data.head(5)\n    .filter([\"name\", \"year\", \"Total\"])\n    .to_markdown(index=False)\n)\n\n\n| name   |   year |   Total |\n|:-------|-------:|--------:|\n| Luke   |   1977 |    1235 |\n| Luke   |   1980 |    3108 |\n| Luke   |   1983 |    2638 |"
  },
  {
    "objectID": "Cleansing_Exploration/project1.html#stretch-question",
    "href": "Cleansing_Exploration/project1.html#stretch-question",
    "title": "Client Report - What’s in a name?",
    "section": "STRETCH QUESTION",
    "text": "STRETCH QUESTION\nANALYZE DATA ABOUT ELLIOT’S NAME, A POPULAR CHARACTER IN E.T.\nE.T. is a beloved series of movies, with releases in 1982, 1985, and 2002. Elliot, the main character in the original 1982 film, became an iconic figure associated with the story. The name “Elliot” saw moderate popularity with 363 instances in 1982 and a slight increase to 398 by 1985. By 2002, the name had grown to 503 instances. However, it wasn’t until 1999 that the name “Elliot” started to gain significant traction, with its popularity rising steadily thereafter. The enduring appeal of the character, coupled with the nostalgia surrounding the E.T. series, likely contributed to this upward trend.\n\n\nCode to analyze Elliot’s name from E.T.\n# choose name, and year range for Elliot's name\nmovie_name = \"Elliot\"\nyear_range = \"year &gt;= 1950 and year &lt;= 2015\"\nelliot_data = names.query(f\"name == '{movie_name}' and {year_range}\")\n\n# create line chart for Elliot's name through the years\nelliot_chart = px.line(\n    elliot_data,\n    x=\"year\",\n    y=\"Total\",\n    color=\"name\",\n    labels={\"year\": \"Year\"},\n    title=\"Elliot...What?\",\n)\n\n# data to display movies released data, 1st , 2nd and 3rd\nelliot_selected_years_text = {\n    1982: \"E.T. Released\",\n    1985: \"Second Released\",\n    2002: \"Third Released\",\n}\net_movie_release_year = [1982, 1985, 2002]\nelliot_selected_data = names.query(\n    f\"year in @et_movie_release_year and name == '{movie_name}'\"\n)\ntotals_movie_year = elliot_selected_data[\"Total\"].values\n\n# add vertical lines for the years when the movies were released\nvlines_loop_count = 0\nselected_years_count = len(elliot_selected_years_text)\nannotation_position_list = [\"top left\", \"top right\", \"top right\"]\n\nwhile vlines_loop_count &lt;= selected_years_count:\n    i = 0\n    for year, text in elliot_selected_years_text.items():\n        # https://plotly.com/python/horizontal-vertical-shapes/\n        elliot_chart.add_vline(\n            x=year,\n            line_width=1,\n            line_dash=\"dash\",\n            line_color=\"red\",\n            annotation_text=text,\n            annotation_position=f\"{annotation_position_list[i]}\",\n        )\n        vlines_loop_count += 1\n        i += 1\n        \nelliot_chart.show()\n\n\n                                                \n\n\nRelease year for movies 1, 2 and 3 are highlighted..\n\n\nTable of years\nprint(\n    elliot_selected_data.head(5)\n    .filter([\"name\", \"year\", \"Total\"])\n    .to_markdown(index=False)\n)\n\n\n| name   |   year |   Total |\n|:-------|-------:|--------:|\n| Elliot |   1982 |     363 |\n| Elliot |   1985 |     398 |\n| Elliot |   2002 |     503 |"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - Late Flights & Missing Data",
    "section": "",
    "text": "Delayed flights are not something most people look forward to. In the best case scenario you may only wait a few extra minutes for the plane to be cleaned. However, those few minutes can stretch into hours if a mechanical issue is discovered or a storm develops. Arriving hours late may result in you missing a connecting flight, job interview, or your best friend’s wedding.\nIn 2003 the Bureau of Transportation Statistics (BTS) began collecting data on the causes of delayed flights. The categories they use are Air Carrier, National Aviation System, Weather, Late-Arriving Aircraft, and Security. You can visit the BTS website to read definitions of these categories.\n\n\nRead and format project data\n# url to data\nflights_url = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\n# read data as JSON file\nflights = pd.read_json(flights_url)"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#elevator-pitch",
    "href": "Cleansing_Exploration/project2.html#elevator-pitch",
    "title": "Client Report - Late Flights & Missing Data",
    "section": "",
    "text": "Delayed flights are not something most people look forward to. In the best case scenario you may only wait a few extra minutes for the plane to be cleaned. However, those few minutes can stretch into hours if a mechanical issue is discovered or a storm develops. Arriving hours late may result in you missing a connecting flight, job interview, or your best friend’s wedding.\nIn 2003 the Bureau of Transportation Statistics (BTS) began collecting data on the causes of delayed flights. The categories they use are Air Carrier, National Aviation System, Weather, Late-Arriving Aircraft, and Security. You can visit the BTS website to read definitions of these categories.\n\n\nRead and format project data\n# url to data\nflights_url = \"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\"\n# read data as JSON file\nflights = pd.read_json(flights_url)"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#task-1",
    "href": "Cleansing_Exploration/project2.html#task-1",
    "title": "Client Report - Late Flights & Missing Data",
    "section": "TASK 1",
    "text": "TASK 1\nCREATE CONSISTENCY BY FIXING MISSING DATA TYPES.\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.\n\n\nFill missing values to display NaN\n# run this cell and avoid all the other cleansing for missing values in general\nmissing_values = [None, \"NA\", \"N/A\", \"na\", \"n/a\", \"null\", \"\", \" \", np.nan]\nflights.replace(missing_values, np.nan, inplace=True)\n\nflights.tail()\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\n\n\n\n\n919\nIAD\nWashington, DC: Washington Dulles International\nDecember\n2015.0\n2799\n182\n183\n61\n0\n17\n443\nNaN\n15438\n2826.0\n0\n1825\n31164\n\n\n920\nORD\nChicago, IL: Chicago O'Hare International\nDecember\n2015.0\n25568\n923\n1755\n1364\n11\n180\n4233\n80962.0\n132055\n72045.0\n435\n22459\n307956\n\n\n921\nSAN\nSan Diego, CA: San Diego International\nNaN\n2015.0\n6231\n480\n606\n256\n5\n37\n1383\n25402.0\n35796\n9038.0\n161\n2742\n73139\n\n\n922\nSFO\nSan Francisco, CA: San Francisco International\nDecember\n2015.0\n13833\n757\n1180\n2372\n9\n147\n4465\n55283.0\n96703\n193525.0\n285\n13788\n359584\n\n\n923\nSLC\nSalt Lake City, UT: Salt Lake City International\nDecember\n2015.0\n8804\n483\n796\n404\n5\n56\n1745\n37354.0\n49549\n13515.0\n158\n6693\n107269\n\n\n\n\n\n\n\nCheck row 921, empty value is displayed as NaN."
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#advanced-data-cleansing-procedures",
    "href": "Cleansing_Exploration/project2.html#advanced-data-cleansing-procedures",
    "title": "Client Report - Late Flights & Missing Data",
    "section": "ADVANCED DATA CLEANSING PROCEDURES",
    "text": "ADVANCED DATA CLEANSING PROCEDURES\nThe following code cleans the DataFrame by addressing various columns:\n\nairport_name: Assigns airport names based on corresponding airport codes.\nmonth: Replaces ‘n/a’ values with NA and fills missing values using forward fill.\nyear: Fills missing values using forward fill.\nnum_of_delays_carrier: Replaces ‘1500+’ with the mean value of the column.\nnum_of_delays_late_aircraft: Replaces ‘-999’ with the mean value of the column.\nminutes_delayed_carrier: Fills missing values with the column mean.\nminutes_delayed_nas: Replaces ‘-999’ with the column mean and fills missing values with the mean.\nNaN check: Checks for any remaining NaN values in the DataFrame.\n\nIf the answer to the following question is True, then the cleasing hasn’t worked correctly.\n\n\nCode to clean each column of the dataframe\n# ***clean airport_name column***\n# this code will assign airport name based on airport codes\nairport_names = [\n    \"Atlanta, GA: Hartsfield-Jackson Atlanta International\",\n    \"Denver, CO: Denver International\",\n    \"Washington, DC: Washington Dulles International\",\n    \"Chicago, IL: Chicago O'Hare International\",\n    \"San Francisco, CA: San Francisco International\",\n    \"San Diego, CA: San Diego International\",\n    \"Salt Lake City, UT: Salt Lake City International\",\n]\nairport_codes = [\"ATL\", \"DEN\", \"IAD\", \"ORD\", \"SAN\", \"SFO\", \"SLC\"]\ncounter = 0\nwhile counter &lt; flights.shape[0]:\n    # while counter &lt; len(airport_codes):\n    if flights.loc[counter, \"airport_code\"] == airport_codes[0]:\n        flights.at[counter, \"airport_name\"] = airport_names[0]\n    elif flights.loc[counter, \"airport_code\"] == airport_codes[1]:\n        flights.at[counter, \"airport_name\"] = airport_names[1]\n    elif flights.loc[counter, \"airport_code\"] == airport_codes[2]:\n        flights.at[counter, \"airport_name\"] = airport_names[2]\n    elif flights.loc[counter, \"airport_code\"] == airport_codes[3]:\n        flights.at[counter, \"airport_name\"] = airport_names[3]\n    elif flights.loc[counter, \"airport_code\"] == airport_codes[4]:\n        flights.at[counter, \"airport_name\"] = airport_names[4]\n    elif flights.loc[counter, \"airport_code\"] == airport_codes[5]:\n        flights.at[counter, \"airport_name\"] = airport_names[5]\n    elif flights.loc[counter, \"airport_code\"] == airport_codes[6]:\n        flights.at[counter, \"airport_name\"] = airport_names[6]\n    counter += 1\n\n# ***clean month column***\nflights[\"month\"].value_counts()\n# replace all n/a values with NA\nflights[\"month\"] = flights[\"month\"].replace(\"n/a\", pd.NA)\n# flights.replace(\"n/a\", np.nan, inplace=True)\n# check for missing values\nflights[\"month\"].isna().value_counts()\n# The \"ffill\" method fills in missing values by forward-filling, which means\n# that it uses the last known value to fill in subsequent missing values.\nflights[\"month\"].fillna(method=\"ffill\", inplace=True)\n\n# ***clean year column***\nflights[\"year\"].fillna(method=\"ffill\", inplace=True)\n\n# ***clean num_of_delays_carrier column***\nflights[\"num_of_delays_carrier\"].value_counts()\ndelays_subset = flights[\"num_of_delays_carrier\"].replace(\"1500+\", pd.NA)\ndelays_subset.dropna()\n# convert column to numeric values and get the mean\ndelays_subset_numeric = pd.to_numeric(delays_subset, errors=\"coerce\")\nmean_delay = round(delays_subset_numeric.mean())\n# add the mean of the column to replace those cells with a 1500+ value\nflights[\"num_of_delays_carrier\"] = flights[\"num_of_delays_carrier\"].replace(\n    \"1500+\", mean_delay)\n\n# ***clean num_of_delays_late_aircraft column***\nflights[\"num_of_delays_late_aircraft\"].value_counts()\ndelays_late_subset = flights[\"num_of_delays_late_aircraft\"].replace(\"-999\", pd.NA)\ndelays_late_subset.dropna()\n# convert column to numeric values and get the mean\ndelays_late_subset_numeric = pd.to_numeric(delays_late_subset, errors=\"coerce\")\nmean_delays_late = round(delays_late_subset_numeric.mean())\n# add the mean of the column to replace those cells with a -999 value\nflights[\"num_of_delays_late_aircraft\"] = flights[\"num_of_delays_late_aircraft\"].replace(\n    -999, mean_delays_late)\n\n# ***clean minutes_delayed_Carrier column***\nflights[\"minutes_delayed_carrier\"].value_counts()\nmin_delayed_carrier_mean = round(flights[\"minutes_delayed_carrier\"].mean())\nflights[\"minutes_delayed_carrier\"].fillna(min_delayed_carrier_mean, inplace=True)\n\n# ***clean minutes_delayed_nas column***\nflights[\"minutes_delayed_nas\"].isna().value_counts()\nmins_delayed_nas_subset = flights[\"minutes_delayed_nas\"].replace(\"-999\", pd.NA)\nmins_delayed_nas_subset.dropna()\nmins_delayed_nas_mean = round(mins_delayed_nas_subset.mean())\n# add the mean of the column to replace those cells with a -999 value\nflights[\"minutes_delayed_nas\"] = flights[\"minutes_delayed_nas\"].replace(\n    -999, mins_delayed_nas_mean)\nflights[\"minutes_delayed_nas\"].fillna(mins_delayed_nas_mean, inplace=True)\n\n# ***check if there's any NaN value in the df***\nhas_nan = flights.isnull().any().any()\nprint(\"Does the DataFrame have any NaN values?\", has_nan)\n\n# ***check if there's any NaN values in the df using columns***\n# nan_columns = flights.isnull().any()\n# print(\"Columns with NaN values:\\n\", nan_columns)\n\n\nDoes the DataFrame have any NaN values? False"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#task-2",
    "href": "Cleansing_Exploration/project2.html#task-2",
    "title": "Client Report - Late Flights & Missing Data",
    "section": "TASK 2",
    "text": "TASK 2\nWhich airport has the worst delays?\nTo determine the “worst” airport for delays, I chose the metric of the proportion of delayed flights. This metric is calculated as the total number of delayed flights divided by the total number of flights for each airport. The reason for choosing this metric is that it directly indicates the likelihood of a flight being delayed at a given airport, providing a clear measure of delay performance. A higher delay proportion signifies a greater frequency of delays, which is more impactful to passengers and operations compared to just the total number of delays or average delay time. Among the listed airports, San Francisco International Airport (SFO) has the highest delay proportion at 26.10%, making it the “worst” airport in terms of flight delays.\n\n\nUse data to display the airports with the worst delays\n# get total flights for each airport\nflight_totals = flights.groupby(\"airport_code\")[\"num_of_flights_total\"].sum()\n\n# get total delays for each airport\ndelayed_totals = flights.groupby(\"airport_code\")[\"num_of_delays_total\"].sum()\n\n# get proportion of delayed flights\ndelayed_proportion = (delayed_totals / flight_totals) * 100\ndelayed_proportion = round(delayed_proportion, 2)\n\n# get minutes delay time in hours\ndelay_totals_minutes = flights.groupby(\"airport_code\")[\"minutes_delayed_total\"].sum()\ndelay_totals_hours = round(delay_totals_minutes / 60, 2)\n\n# create the worst airport data frame\nworst_df = pd.DataFrame(\n    {\n        # \"AirportCode\": flight_totals.index,\n        \"TotalFlights\": flight_totals,\n        \"TotalDelays\": delayed_totals,\n        \"DelayProportion\": delayed_proportion,\n        \"DelayTime(Hours)\": delay_totals_hours,\n    }\n)\nworst_df = worst_df.sort_values(by=\"DelayTime(Hours)\", ascending=False)\nworst_df\n\n\n\n\n\n\n\n\n\nTotalFlights\nTotalDelays\nDelayProportion\nDelayTime(Hours)\n\n\nairport_code\n\n\n\n\n\n\n\n\nORD\n3597588\n830825\n23.09\n939268.82\n\n\nATL\n4430047\n902443\n20.37\n899732.10\n\n\nSFO\n1630945\n425604\n26.10\n442508.22\n\n\nDEN\n2513974\n468519\n18.64\n419556.35\n\n\nIAD\n851571\n168467\n19.78\n171391.30\n\n\nSLC\n1403384\n205160\n14.62\n168722.85\n\n\nSAN\n917862\n175132\n19.08\n137937.47\n\n\n\n\n\n\n\nSan Francisco International Airport (SFO) has the highest delay proportion with 26.10%, Chicago’s O’Hare International Airport is in second place with a 23.09%."
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#task-3",
    "href": "Cleansing_Exploration/project2.html#task-3",
    "title": "Client Report - Late Flights & Missing Data",
    "section": "TASK 3",
    "text": "TASK 3\nWHAT IS THE BEST MONTH TO FLY IF YOU WANT TO AVOID DELAYS OF ANY LENGTH?\nTo determine the best month to fly if you want to avoid delays of any length, I chose the metric of total minutes delayed per month. This metric reflects the overall delay time experienced by flights in each month, providing a clear indicator of which months tend to have fewer delays. After analyzing the data from 2005 to 2015, September emerged as the best month to fly, with the lowest total minutes delayed at 152,221.09. This indicates that flights in September generally experience the least amount of delay time, making it the optimal month for travelers seeking to avoid delays.\n\n\nCode to analyze average delay time from 2005 to 2015\n# average delay time per each month, reset index and convert to dataframe\nflights_month = (\n    flights.groupby(\"month\")[\"minutes_delayed_total\"].mean().round(2).reset_index()\n)\n\nfig = px.scatter(\n    flights_month,\n    x=\"month\",\n    y=\"minutes_delayed_total\",\n    title=\"Avg. delay time (mins) by month (2005-2015)\",\n    labels={\"minutes_delayed_total\": \"Average Minutes\", \"month\": \"Month\"},\n)\n\n# format amounts to show (,) and round them, display each 30,000\nfig.update_yaxes(tickformat=\",.0f\", dtick=30000)\nfig.show()\n\n\n                                                \n\n\nSeptember is the best month to fly, with the lowest total minutes delayed at 152,221.09."
  },
  {
    "objectID": "Cleansing_Exploration/project2.html#task-4",
    "href": "Cleansing_Exploration/project2.html#task-4",
    "title": "Client Report - Late Flights & Missing Data",
    "section": "TASK 4",
    "text": "TASK 4\nLATE FLIGHTS BECAUSE OF WEATHER\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. A new column were created, it calculates the total number of flights delayed by weather (both severe and mild). All the missing values in the Late Aircraft variable were replaced with the mean value. The first 5 rows of data are shown in a table to demonstrate this calculation. These three rules were for the calculations:\n\n100% of delayed flights in the Weather category are due to weather\n30% of all delayed flights in the Late-Arriving category are due to weather\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.\n\n\n\nCode to analyze delay flights by weather\n# total flights for each airport\nflight_totals = flights.groupby(\"airport_code\")[\"num_of_flights_total\"].sum()\n\n# total delays for each airport\ndelayed_totals = flights.groupby(\"airport_code\")[\"num_of_delays_total\"].sum()\n\n# RULE 1: 100% of delayed flights in the Weather category are due to weather\nweather_100 = flights.groupby(\"airport_code\")[\"num_of_delays_weather\"].sum()\n\n# RULE 2: 30% of all delayed flights in the Late-Arriving category are due to weather\nsample_flights_30 = flights.sample(frac=0.3, random_state=1)\nweather_30 = sample_flights_30.groupby(\"airport_code\")[\n    \"num_of_delays_late_aircraft\"\n].sum()\n\n# RULE 3: From April to August, 40% of delayed flights in the NAS category are due to weather.\nmonths_40 = [\"April\", \"May\", \"June\", \"July\", \"August\"]\nmonths_65 = [\n    \"January\",\n    \"February\",\n    \"March\",\n    \"September\",\n    \"October\",\n    \"November\",\n    \"December\",\n]\n\ndf_40 = flights.query(f\"month in @months_40\")\nsample_flights_40 = df_40.sample(frac=0.4, random_state=1)\nweather_40 = sample_flights_40.groupby(\"airport_code\")[\"num_of_delays_nas\"].sum()\n\n# RULE 3.1: The rest of the months, the proportion rises to 65%\n\ndf_65 = flights.query(f\"month in @months_65\")\nsample_flights_65 = df_65.sample(frac=0.65, random_state=1)\nweather_65 = sample_flights_65.groupby(\"airport_code\")[\"num_of_delays_nas\"].sum()\n\n\n# dataframe to display the results\nweather_df = pd.DataFrame(\n    {\n        \"TotalFlights\": flight_totals,\n        \"TotalDelays\": delayed_totals,\n        \"WeatherDelay\": weather_100,\n        \"DelaysLateAircraft\": weather_30,\n        \"DelayNAS40\": weather_40,\n        \"DelayNAS65\": weather_65,\n    }\n).reset_index()\n\nweather_df\n\n\n\n\n\n\n\n\n\nairport_code\nTotalFlights\nTotalDelays\nWeatherDelay\nDelaysLateAircraft\nDelayNAS40\nDelayNAS65\n\n\n\n\n0\nATL\n4430047\n902443\n32375\n56109\n56969\n113774\n\n\n1\nDEN\n2513974\n468519\n13836\n41232\n24368\n44908\n\n\n2\nIAD\n851571\n168467\n4794\n21143\n11488\n14227\n\n\n3\nORD\n3597588\n830825\n20765\n88245\n50875\n105548\n\n\n4\nSAN\n917862\n175132\n4320\n20957\n8502\n15181\n\n\n5\nSFO\n1630945\n425604\n10377\n37367\n45907\n65225\n\n\n6\nSLC\n1403384\n205160\n6831\n26889\n9799\n19775\n\n\n\n\n\n\n\nThis table look overwhelming as many numbers are shown.\nWe will summarize the previous table by calculating the proportion of delayed flights due to weather-related factors for each airport in a new DataFrame. A iteration was made through each row of the DataFrame, calculated the total delay time, divided it by the total delayed flights, and converted it to a percentage. The resulting proportions are displayed in a chart with columns for airport codes and their respective delay proportions due to weather.\n\n\nCode to analyze delay flights by weather\ncounter_weather = 0\nproportion_delays_totals = []\nairtport_codes_weather = []\nwhile counter_weather &lt; len(weather_df[\"TotalFlights\"]):\n\n    # make the sum for the rows\n    rule1_percent = weather_df.WeatherDelay[counter_weather]\n    rule2_percent = weather_df.DelaysLateAircraft[counter_weather]\n    rule3_percent = weather_df.DelayNAS40[counter_weather]\n    rule31_percent = weather_df.DelayNAS65[counter_weather]\n    # perform the sum of each cell of the row\n    row_total = rule1_percent + rule2_percent + rule3_percent + rule31_percent\n    # get elements of the total delays due of weather column\n    flight_totals_weather = weather_df.TotalDelays[counter_weather]\n    # get proportion of delayed flights due of weather and append the\n    # proportion to the list proportion delays total\n    delayed_proportion_weather = (row_total / flight_totals_weather) * 100\n    delayed_proportion_weather = round(delayed_proportion_weather, 1)\n    # print(delayed_proportion_weather)\n    proportion_delays_totals.append(delayed_proportion_weather)\n    # get airport codes and append them to the list of airport codes\n    airport_code_weather = weather_df.airport_code[counter_weather]\n    airtport_codes_weather.append(airport_code_weather)\n    # create new data frame\n    weather_proportions_data = {\n        \"airport_code\": airtport_codes_weather,\n        \"proportion\": proportion_delays_totals,\n    }\n    proportions_weather_df = pd.DataFrame(weather_proportions_data)\n    counter_weather += 1\n\nproportions_weather_df\n\n\n\n\n\n\n\n\n\nairport_code\nproportion\n\n\n\n\n0\nATL\n28.7\n\n\n1\nDEN\n26.5\n\n\n2\nIAD\n30.7\n\n\n3\nORD\n31.9\n\n\n4\nSAN\n28.0\n\n\n5\nSFO\n37.3\n\n\n6\nSLC\n30.9\n\n\n\n\n\n\n\nSan Francisco International Airport (SFO) has the highest proportion of delayed flights because of weather at 37.3%, making it the worst for delays also in this category. Chicago O’Hare International Airport (ORD) follows with a 31.9% delay proportion. This indicates that travelers using SFO and ORD are more likely to experience delays due of weather compared to other airports.\n\n\nCode to display chart of delayed flights due of weather\npropotions_chart = px.bar(\n    proportions_weather_df,\n    x=\"airport_code\",\n    y=\"proportion\",\n    title=\"Proportion of delays due of weather by airport\",\n    labels={\"airport_code\": \"Airport Code\", \"proportion\": \"Proportion %\"},\n)\npropotions_chart.show()\n\n\n                                                \n\n\nWashington Dulles International Airport (IAD) and Salt Lake City International Airport (SLC) also have high delay proportions at around 30%. Overall, SFO and ORD stand out as the airports with the most significant delay issues."
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - Can you predict that?",
    "section": "",
    "text": "The clean air act of 1970 was the beginning of the end for the use of asbestos in home building. By 1976, the U.S. Environmental Protection Agency (EPA) was given authority to restrict the use of asbestos in paint. Homes built during and before this period are known to have materials with asbestos. You can read more about this ban.\nThe state of Colorado has a large portion of their residential dwelling data that is missing the year built and they would like to build a predictive model that can classify if a house is built pre 1980.\nColorado provided home sales data for the city of Denver from 2013 on which to train the model. They said all the column names should be descriptive enough for the modeling and that they would like to use the latest machine learning methods.\n\n\nRead and format project data\ndata_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\ndata_ml = pd.read_csv(data_url)"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#elevator-pitch",
    "href": "Cleansing_Exploration/project4.html#elevator-pitch",
    "title": "Client Report - Can you predict that?",
    "section": "",
    "text": "The clean air act of 1970 was the beginning of the end for the use of asbestos in home building. By 1976, the U.S. Environmental Protection Agency (EPA) was given authority to restrict the use of asbestos in paint. Homes built during and before this period are known to have materials with asbestos. You can read more about this ban.\nThe state of Colorado has a large portion of their residential dwelling data that is missing the year built and they would like to build a predictive model that can classify if a house is built pre 1980.\nColorado provided home sales data for the city of Denver from 2013 on which to train the model. They said all the column names should be descriptive enough for the modeling and that they would like to use the latest machine learning methods.\n\n\nRead and format project data\ndata_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\ndata_ml = pd.read_csv(data_url)"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#question-1",
    "href": "Cleansing_Exploration/project4.html#question-1",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION 1",
    "text": "QUESTION 1\nEVALUATE POTENTIAL RELATIONSHIPS BETWEEN THE HOME VARIABLES AND BEFORE1980\nIn the following charts we are going to analyze different features that can help the model to better classify if houses were built before or after 1980.\n\n\nCode mean of living area chart\n# mean of living area through the years\nafter_1900 = data_ml.query(\"yrbuilt &gt;= 1900\")\nmean_area_per_year = after_1900.groupby(\"yrbuilt\")[\"livearea\"].mean().reset_index()\nmean_area_per_year_chart = px.line(\n    mean_area_per_year,\n    x=\"yrbuilt\",\n    y=\"livearea\",\n    labels={\"yrbuilt\": \"Year Built\", \"livearea\": \"Live Area\"},\n    title=\"Mean Living Area Through the Years\",\n)\nmean_area_per_year_chart.add_vline(\n    x=1980,\n    line_width=1,\n    line_dash=\"dash\",\n    line_color=\"red\",\n    annotation_text=\"1980\",\n)\nmean_area_per_year_chart.show()\n\n\n                                                \n\n\nDid the living area changed through the years? Through the years the mean living area were increasing. During 1980 it decreased considerably, but it increased again the following years.\n\n\nCode mean of bedrooms chart\n# mean of bedrooms through the years\nmean_bedrooms_per_year = after_1900.groupby(\"yrbuilt\")[\"numbdrm\"].mean().reset_index()\nbedrooms_year_chart = px.scatter(\n    mean_bedrooms_per_year,\n    x=\"yrbuilt\",\n    y=\"numbdrm\",\n    labels={\"yrbuilt\": \"Year Built\", \"numbdrm\": \"Number of bedrooms\"},\n    title=\"Mean of Bedrooms Through the Years\",\n)\nbedrooms_year_chart.add_vline(\n    x=1980,\n    line_width=1,\n    line_dash=\"dash\",\n    line_color=\"red\",\n    annotation_text=\"1980\",\n)\nbedrooms_year_chart.show()\n\n\n                                                \n\n\nDid the number of bedrooms increased or decreased through the years? The mean of bedrooms was changing a lot through the years, it may not be the best criteria in this case. Because some years it increased and others decreased, not a clear pattern is shown.\n\n\nCode mean average price chart\n# mean average price through the years\nxx_century = data_ml.query(\"yrbuilt &gt;= 1900 and yrbuilt &lt; 2000\")\nprice_per_year = xx_century.groupby(\"yrbuilt\")[\"sprice\"].mean().reset_index()\nprice_per_year_chart = px.bar(\n    price_per_year,\n    x=\"yrbuilt\",\n    y=\"sprice\",\n    labels={\"yrbuilt\": \"Year Built\", \"sprice\": \"Selling Price\"},\n    title=\"Mean Average Selling Prices in the XX Century\",\n)\nprice_per_year_chart.add_vline(\n    x=1980,\n    line_width=1,\n    line_dash=\"dash\",\n    line_color=\"red\",\n    annotation_text=\"1980\",\n)\nprice_per_year_chart.show()\n\n\n                                                \n\n\nMean Average selling prices in the XX century decreased and maintained certain stability for many years, around 1984 prices increased and decreased again showing a considerable variance each year. 1980 may be the year with the lowest prices."
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#question-2",
    "href": "Cleansing_Exploration/project4.html#question-2",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION 2",
    "text": "QUESTION 2\nBUILD CLASSIFICATION MODEL LABELING HOUSES ACCORDING TO THE BUILT YEAR\nlet’s build a classification model labeling houses as being built “before 1980” or “during or after 1980”. The goal is to reach or exceed 90% accuracy.\nTwo clasffication models were built, a logistic regression and random forest. Both models shew high accuracy, indicating that the models are built correctly or that some adjustments need to be made to the collected data.\nAccording to the results the Random Forest model achieved a higher accuracy (93%) compared to the Logistic Regression model (88%) for predicting whether a house was built before or after 1980.\n\n\nCode to build ML models\n# drop column yrbuilt to avoid overfitting\ndata_ml.drop(columns=[\"yrbuilt\"], inplace=True)\n\n# define features (X) and target variable (y)\nX = data_ml.drop(\n    columns=[\n        \"before1980\",\n        \"parcel\",\n    ]\n)\ny = data_ml[\"before1980\"]\n\n# split data, training and testing\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# train logistic regression model\nlogreg_model = LogisticRegression(max_iter=1000)\nlogreg_model.fit(X_train_scaled, y_train)\n\n# train random forest classifier model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\n\n# model evaluation\ny_pred_logreg = logreg_model.predict(X_test_scaled)\ny_pred_rf = rf_model.predict(X_test_scaled)\n\naccuracy_logreg = accuracy_score(y_test, y_pred_logreg)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\n\nprint(f\"Logistic Regression Accuracy: {accuracy_logreg:.2f}\")\nprint(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n\n# classification reports comparisson\nprint(\"\\nLogistic Regression Classification Report:\")\nprint(\n    classification_report(\n        y_test, y_pred_logreg, target_names=[\"After 1980\", \"Before 1980\"]\n    )\n)\n\nprint(\"\\nRandom Forest Classification Report:\")\nprint(\n    classification_report(y_test, y_pred_rf, target_names=[\"After 1980\", \"Before 1980\"])\n)\n\n\nLogistic Regression Accuracy: 0.88\nRandom Forest Accuracy: 0.93\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n  After 1980       0.85      0.81      0.83      1710\n Before 1980       0.89      0.92      0.90      2873\n\n    accuracy                           0.88      4583\n   macro avg       0.87      0.86      0.87      4583\nweighted avg       0.87      0.88      0.87      4583\n\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n  After 1980       0.91      0.90      0.91      1710\n Before 1980       0.94      0.95      0.94      2873\n\n    accuracy                           0.93      4583\n   macro avg       0.93      0.93      0.93      4583\nweighted avg       0.93      0.93      0.93      4583"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#question-3",
    "href": "Cleansing_Exploration/project4.html#question-3",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION 3",
    "text": "QUESTION 3\nJUSTIFY CLASSIFICATION MODELS\nRandom Forest model is good at identifying true positives (houses built after 1980) - 91% of the time it predicts a house is built after. It also identifies true negatives correctly (houses built before 1980) - 94% of the time it predicts a house is built before 1980.\nThe model is also good at recalling both positives (houses built after 1980) with a 90% of accuracy and negatives (houses built before 1980) with a 95% of accuracy.\nLiving area (livearea column) seems to be the most important feature in the model, other important features are number of bathrooms, net price, selling price and square footage of the basement.\n\n\nCode feature importance\n# get feature importance from the random forest model\nfeature_importance = rf_model.feature_importances_\n\nimportance_df = pd.DataFrame({\"Feature\": X.columns, \"Importance\": feature_importance})\nimportance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\nimportance_df = importance_df.head(10)\n\n# create chart to display feature importance from model\nfeature_importance_chart = px.bar(\n    importance_df,\n    x=\"Importance\",\n    y=\"Feature\",\n    title=\"Feature Importance from Random Forest Model\",\n)\n\nfeature_importance_chart.show()\n\n\n                                                \n\n\nFeature importance in ascending order."
  },
  {
    "objectID": "Cleansing_Exploration/project4.html#question-4",
    "href": "Cleansing_Exploration/project4.html#question-4",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION 4",
    "text": "QUESTION 4\nEVALUATION METRICS\nLet’s review important evaluation metrics to consider when evaluating the quality of a classification mode, in this case the random forest model.\nAccuracy: Measures the proportion of correctly classified instances. For Random Forest, it is 93%.High accuracy indicates the model performs well overall. The goal was to get at least 90% of accuracy, so had a higher accuracy than expected.\nPrecision: Indicates the proportion of true positive predictions among all positive predictions. For “Before 1980”, it is 0.94. High precision means fewer false positives.\nRecall: Measures the proportion of true positive predictions among all actual positives. For “Before 1980”, it is 0.95. High recall means fewer false negatives.\nF1-Score: Harmonic mean of precision and recall, providing a balance between the two. For “Before 1980”, it is 0.94. A high F1-score indicates a good balance between precision and recall."
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 1"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 1"
    ]
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 3"
    ]
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 5"
    ]
  }
]